{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skeleton Notebook Deep Q-Learning Project (MHBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/labmlai/annotated_deep_learning_paper_implementations?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!pip install -q gym==0.15.4\n",
    "#!pip install -q pycolab==1.2\n",
    "#!pip install -q torch==1.2.0\n",
    "#!pip install -q matplotlib==3.1.2\n",
    "#!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import required packages\n",
    "import gym\n",
    "import gym_grid\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Environment Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAABcCAYAAADpqcO+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAChUlEQVR4nO3ZoU2EMRiA4f8IyzAKwaCwOBQDQE5fwgAoRkBhCAmCLQiDMEBR6DvxlstPnkc37VdR8aabMcZYAAAAQifHHgAAAPh/hAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAudNDF97dbmfOAQAArMTD427vGj8aAABATmgAAAA5oQEAAOSEBgAAkBMaAABATmgAAAA5oQEAAOSEBgAAkBMaAABATmgAAAA5oQEAAOSEBgAAkBMaAABATmgAAAA5oQEAAOSEBgAAkBMaAABATmgAAAA5oQEAAOSEBgAAkBMaAABATmgAAAA5oQEAAOROjz3Ar+uLz6n7n52/TN1/WZbl6+1y6v6z77D2+Zdl/Xcw/35rv8NfvIOZnq4upp9x8/w6/QyAY3v/nrv/x/127gEH8KMBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAOaEBAADkhAYAAJATGgAAQE5oAAAAuc0YYxyy8O52O3sWAABgBR4ed3vX+NEAAAByQgMAAMgJDQAAICc0AACAnNAAAAByQgMAAMgJDQAAICc0AACAnNAAAAByQgMAAMgJDQAAICc0AACAnNAAAAByQgMAAMgJDQAAICc0AACAnNAAAAByQgMAAMgJDQAAICc0AACAnNAAAAByQgMAAMgJDQAAILcZY4xjDwEAAPwvfjQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAICc0AAAAHJCAwAAyAkNAAAgJzQAAIDcD2QQLa4waQoKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the environment\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# T-Maze Environment\n",
    "env_lin = gym.make(\"LinearTrack-v0\")\n",
    "_, obs_to_render = env_lin.reset_with_render()\n",
    "env_lin.reset()\n",
    "env_lin.render(obs_to_render)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Deadly Gridworld\n",
    "env_grid = gym.make(\"DeadlyGrid-v0\")\n",
    "_, obs_to_render = env_grid.reset_with_render()\n",
    "env_grid.reset()\n",
    "env_grid.render(obs_to_render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAADoCAYAAAByx+c/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAERklEQVR4nO3YPU5CURRGUTHMzt4haMMASF5NwgBeg0Owd3zXjsQK/CE3steqT/GVO2czxhgPAEDW4+wBAMBcYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgLjttYf73XLLHQDADRzXw8UbnwEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABC3nT3gJ47rYfYE+JXd4XX2hC/W5TR7AtyN/W6ZPeHbfAYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAuO3sAVC0LqfZEwDOfAYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxG1nD7gHb89Psyecvbx/zJ4AwD/jMwAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOI2Y4xxzeF+t9x6CwDwx47r4eKNzwAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQtxljjNkjAIB5fAYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIO4T24QecgVIWR0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run random steps & Visualize the episode\n",
    "from IPython import display\n",
    "\n",
    "_, obs_to_render = env_grid.reset_with_render()\n",
    "env_grid.render(obs_to_render)\n",
    "rew_total = 0\n",
    "\n",
    "i = 0\n",
    "actions = [4] + [2 for i in range(11)]+[1]+[3 for i in range(11)]+[1]+[2 for i in range(11)]+[1]+[3 for i in range(11)]+[1]\n",
    "\n",
    "while True:\n",
    "    #action = env.action_space.sample()\n",
    "    action = actions[i]\n",
    "    i+=1\n",
    "    _, rew , done, _, obs_to_render = env_grid.step_with_render(action)\n",
    "    env_grid.render(obs_to_render)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    rew_total += rew\n",
    "    if done:\n",
    "        break\n",
    "print(rew_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Deep Q-Learning Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim=384, output_dim=3):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer_in = nn.Linear(input_dim, 128)\n",
    "        self.hidden_layer = nn.ReLU()\n",
    "        self.layer_out = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_in(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.layer_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFuncLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, q, action,\n",
    "                target_q, done, reward):\n",
    "        \n",
    "\n",
    "        #q_sampled_action = q.gather(-1, action.to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            best_next_action = torch.argmax(q, -1)\n",
    "            best_next_q_value = target_q.gather(-1, best_next_action.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "            q_update = reward + self.gamma * best_next_q_value * (1 - done)\n",
    "\n",
    "            td_error = q - q_update\n",
    "\n",
    "        losses = self.mse_loss(q, q_update)\n",
    "\n",
    "        loss = torch.mean(losses)\n",
    "\n",
    "        return td_error, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, epochs, lr, eps, env, in_size=384, out_size=3):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = lr\n",
    "        self.epsilon = eps\n",
    "        self.model = DQN(input_dim=in_size, output_dim=out_size)\n",
    "        self.target_model = DQN(input_dim=in_size, output_dim=out_size)\n",
    "        \n",
    "        self.state = torch.FloatTensor(env.reset()).flatten().unsqueeze(0)\n",
    "        \n",
    "        self.loss_func = QFuncLoss(0.01) \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate) \n",
    "\n",
    "    def epsilon_greedy_policy(self, q_value, epsilon):\n",
    "        with torch.no_grad():\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.randint(0, q_value.shape[-1])\n",
    "            else:\n",
    "                action = torch.argmax(q_value, dim=-1)\n",
    "            return action\n",
    "\n",
    "    def sample(self, epsilon, state):\n",
    "        with torch.no_grad():\n",
    "            q_value = self.model(state)\n",
    "            action = self.epsilon_greedy_policy(q_value, epsilon)\n",
    "            return q_value, action\n",
    "\n",
    "    def train(self):\n",
    "        for _ in range(self.epochs):\n",
    "            with torch.no_grad():\n",
    "                q_value, action = self.sample(self.epsilon, self.state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = torch.FloatTensor(next_state).flatten().unsqueeze(0)\n",
    "                next_q_value, next_action = self.sample(self.epsilon, next_state)\n",
    "                print(next_q_value, q_value)\n",
    "\n",
    "\n",
    "                loss = self.loss_func(q_value, action,\n",
    "                                      next_q_value, done, reward)\n",
    "\n",
    "                print(loss)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0382,  0.0232, -0.0730]]) tensor([[-0.0654,  0.0192, -0.0477]])\n",
      "(tensor([[-0.0656,  0.0190, -0.0479]]), tensor(0.0023))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train \u001b[38;5;241m=\u001b[39m Training(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.\u001b[39m, env_lin, in_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m384\u001b[39m, out_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[140], line 43\u001b[0m, in \u001b[0;36mTraining.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "train = Training(10, 0.01, 0., env_lin, in_size=384, out_size=3)\n",
    "train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(network, optimizer, loss_function, \n",
    "                   state, action, reward, \n",
    "                   next_state, done, gamma=0.99, eta=0.001):\n",
    "    \n",
    "    #Convert inputs to tensors\n",
    "\n",
    "    state = torch.FloatTensor(state).flatten().unsqueeze(0)\n",
    "    next_state = torch.FloatTensor(next_state).flatten().unsqueeze(0)\n",
    "\n",
    "    # Forward pass to get Q-values\n",
    "    q_values = network(state)\n",
    "\n",
    "    # Get the Q-value for the chosen action\n",
    "    q_value = q_values[0][action].squeeze(0)\n",
    "\n",
    "    # Calculate the target Q-value\n",
    "    with torch.no_grad():\n",
    "        next_q_values = network(next_state).max(1)[0]\n",
    "        q_update = reward + gamma * next_q_values * (1 - done)\n",
    "        td_error = q_value - q_update\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = loss_function(q_value, q_update)\n",
    "    #loss = torch.mean()\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return q_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     23\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m---> 25\u001b[0m update_q_value(q_network, optimizer, loss_function, state, action, reward, \n\u001b[1;32m     26\u001b[0m            next_state, done, gamma, eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 23\u001b[0m, in \u001b[0;36mupdate_q_value\u001b[0;34m(network, optimizer, loss_function, state, action, reward, next_state, done, gamma, eta)\u001b[0m\n\u001b[1;32m     20\u001b[0m     td_error \u001b[38;5;241m=\u001b[39m q_value \u001b[38;5;241m-\u001b[39m q_update\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(q_value, q_update)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#loss = torch.mean()\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3339\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Const\n",
    "# Init env and network\n",
    "eta = 0.001\n",
    "gamma=0.99\n",
    "#q_network = DQN(6*14*5, 5)\n",
    "q_network = DQN()\n",
    "env = env_lin\n",
    "\n",
    "evaluation = []\n",
    "epsilon = 1 #->0\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    state = env.reset()\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=eta) \n",
    "    total_reward = 0\n",
    "    epsilon *= 0.8\n",
    "    while True:\n",
    "    \n",
    "        action = epsilon_greedy_policy(state, epsilon, q_network)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "        update_q_value(q_network, optimizer, loss_function, state, action, reward, \n",
    "                   next_state, done, gamma, eta=0.001)\n",
    "    \n",
    "    \n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "        \n",
    "    if i%500==0:\n",
    "        print(\"Testing\")\n",
    "        total_reward_test = []\n",
    "        for _ in range(10):\n",
    "            trt = 0\n",
    "            j = 0\n",
    "            state = env.reset()\n",
    "            while True:\n",
    "                action = epsilon_greedy_policy(state, 0.01, q_network)\n",
    "                _, rew , done, _ = env.step(action)\n",
    "                trt += gamma**j*rew\n",
    "                if done:\n",
    "                    break\n",
    "                j+=1\n",
    "            total_reward_test.append(trt)\n",
    "        evaluation.append(np.mean(np.array(total_reward_test)))\n",
    "plt.plot(evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, obs_to_render = env_grid.reset_with_render()\n",
    "env_grid.render(obs_to_render)\n",
    "rew_total = 0\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    action = epsilon_greedy_policy(state, epsilon, q_network)\n",
    "    i+=1\n",
    "    _, rew , done, _, obs_to_render = env_grid.step_with_render(action)\n",
    "    env_grid.render(obs_to_render)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    rew_total += rew\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "        \n",
    "print(rew_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
